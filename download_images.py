#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –í–°–ï–• –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–∞–π—Ç–∞ metriks96.ru –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–µ
"""

import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import time
from pathlib import Path
import re
import json

class AdvancedImageDownloader:
    def __init__(self, base_url="https://metriks96.ru"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.downloaded_images = set()
        self.image_folder = Path("downloaded_images")
        self.image_folder.mkdir(exist_ok=True)
        self.visited_urls = set()
        self.all_found_urls = set()
        
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
            return None
    
    def get_high_quality_url(self, image_url):
        """–ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø–æ–ª—É—á–∏—Ç—å URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–µ"""
        # –î–ª—è Tilda (–Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω —Å–∞–π—Ç) —á–∞—Å—Ç–æ –µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
        parsed = urlparse(image_url)
        
        # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∂–∞—Ç–∏—è –∏ —Ä–µ—Å–∞–π–∑–∞
        high_quality_variants = [
            image_url,  # –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL
            re.sub(r'resize=\d+x\d+', '', image_url),  # —É–±–∏—Ä–∞–µ–º resize
            re.sub(r'quality=\d+', 'quality=100', image_url),  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            re.sub(r'format=webp', 'format=jpg', image_url),  # JPG –≤–º–µ—Å—Ç–æ WebP
            re.sub(r'_\d+x\d+\.', '.', image_url),  # —É–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            re.sub(r'thumb_', '', image_url),  # —É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å thumb
            re.sub(r'small_', '', image_url),  # —É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å small
            re.sub(r'medium_', '', image_url),  # —É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å medium
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if '?' in image_url:
            base_url = image_url.split('?')[0]
            high_quality_variants.append(base_url)
        
        return high_quality_variants

    def extract_images_from_html(self, html_content, base_url):
        """–ò–∑–≤–ª–µ—á—å –í–°–ï URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ HTML —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º"""
        soup = BeautifulSoup(html_content, 'html.parser')
        image_urls = set()
        
        # 1. –ù–∞–π—Ç–∏ –≤—Å–µ —Ç–µ–≥–∏ img —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
        for img in soup.find_all('img'):
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            for attr in ['src', 'data-src', 'data-original', 'data-lazy', 'data-srcset', 'srcset']:
                value = img.get(attr)
                if value:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ srcset (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ URL)
                    if 'srcset' in attr:
                        urls = re.findall(r'(https?://[^\s,]+)', value)
                        for url in urls:
                            image_urls.update(self.get_high_quality_url(url))
                    else:
                        full_url = urljoin(base_url, value)
                        image_urls.update(self.get_high_quality_url(full_url))
        
        # 2. –ù–∞–π—Ç–∏ CSS background-image –≤–æ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        for element in soup.find_all():
            style = element.get('style', '')
            if 'background' in style:
                urls = re.findall(r'url\(["\']?([^"\']+)["\']?\)', style)
                for url in urls:
                    full_url = urljoin(base_url, url)
                    if self.is_image_url(full_url):
                        image_urls.update(self.get_high_quality_url(full_url))
        
        # 3. –ù–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ JavaScript –∫–æ–¥–µ
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                # –ü–æ–∏—Å–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ JS
                js_urls = re.findall(r'["\']([^"\']*\.(?:jpg|jpeg|png|gif|webp|svg|bmp|ico)[^"\']*)["\']', script.string, re.IGNORECASE)
                for url in js_urls:
                    full_url = urljoin(base_url, url)
                    image_urls.update(self.get_high_quality_url(full_url))
        
        # 4. –ù–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for element in soup.find_all():
            for attr_name, attr_value in element.attrs.items():
                if 'data-' in attr_name and isinstance(attr_value, str):
                    if self.is_image_url(attr_value):
                        full_url = urljoin(base_url, attr_value)
                        image_urls.update(self.get_high_quality_url(full_url))
        
        # 5. –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∏–Ω–æ–≥–¥–∞ URL –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ)
        text_urls = re.findall(r'https?://[^\s<>"]+\.(?:jpg|jpeg|png|gif|webp|svg|bmp|ico)[^\s<>"]*', html_content, re.IGNORECASE)
        for url in text_urls:
            image_urls.update(self.get_high_quality_url(url))
        
        return image_urls
    
    def is_image_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"""
        image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg', '.ico'}
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        return any(path.endswith(ext) for ext in image_extensions)
    
    def try_download_best_quality(self, image_urls):
        """–ü–æ–ø—ã—Ç–∞—Ç—å—Å—è —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –ª—É—á—à–µ–º –∫–∞—á–µ—Å—Ç–≤–µ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤"""
        for image_url in image_urls:
            if image_url in self.downloaded_images:
                continue
                
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL
                head_response = self.session.head(image_url, timeout=5)
                if head_response.status_code == 200:
                    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    response = self.session.get(image_url, timeout=15)
                    response.raise_for_status()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    content_type = response.headers.get('content-type', '').lower()
                    if not any(img_type in content_type for img_type in ['image/', 'application/octet-stream']):
                        continue
                    
                    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–º—è —Ñ–∞–π–ª–∞
                    parsed_url = urlparse(image_url)
                    filename = os.path.basename(parsed_url.path)
                    if not filename or '.' not in filename:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ content-type
                        ext = '.jpg'
                        if 'png' in content_type:
                            ext = '.png'
                        elif 'gif' in content_type:
                            ext = '.gif'
                        elif 'webp' in content_type:
                            ext = '.webp'
                        filename = f"image_{abs(hash(image_url)) % 100000}{ext}"
                    
                    # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∏–º—è —Ñ–∞–π–ª–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ
                    counter = 1
                    original_filename = filename
                    while (self.image_folder / filename).exists():
                        name, ext = os.path.splitext(original_filename)
                        filename = f"{name}_{counter}{ext}"
                        counter += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª
                    file_path = self.image_folder / filename
                    with open(file_path, 'wb') as f:
                        f.write(response.content)
                    
                    # –û—Ç–º–µ—Ç–∏—Ç—å –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–∞–∫ —Å–∫–∞—á–∞–Ω–Ω—ã–µ
                    for url in image_urls:
                        self.downloaded_images.add(url)
                    
                    size_mb = len(response.content) / (1024 * 1024)
                    print(f"‚úì –°–∫–∞—á–∞–Ω–æ: {filename} ({size_mb:.2f} MB)")
                    return True
                    
            except requests.RequestException:
                continue
        
        print(f"‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –Ω–∏ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ {len(image_urls)} URL")
        return False
    
    def discover_all_pages(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π—Ç–∏ –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–π—Ç–∞"""
        discovered_pages = set()
        pages_to_check = [self.base_url]
        
        while pages_to_check:
            current_url = pages_to_check.pop(0)
            if current_url in self.visited_urls:
                continue
                
            print(f"üîç –ò—Å—Å–ª–µ–¥—É–µ–º: {current_url}")
            self.visited_urls.add(current_url)
            
            html_content = self.get_page_content(current_url)
            if not html_content:
                continue
                
            discovered_pages.add(current_url)
            
            # –ù–∞–π—Ç–∏ –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            soup = BeautifulSoup(html_content, 'html.parser')
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(current_url, href)
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –≤–µ–¥–µ—Ç –Ω–∞ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
                if urlparse(full_url).netloc == urlparse(self.base_url).netloc:
                    if full_url not in self.visited_urls and full_url not in pages_to_check:
                        pages_to_check.append(full_url)
            
            time.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        return list(discovered_pages)

    def get_all_pages(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        pages = [
            self.base_url,
            f"{self.base_url}/catalog",
            f"{self.base_url}/payment", 
            f"{self.base_url}/contacts",
            f"{self.base_url}/politics",
        ]
        
        # –î–æ–±–∞–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤
        categories = [
            "–°–∞–º–æ—Ä–µ–∑—ã", "–í–∏–Ω—Ç—ã", "–ó–∞–∫–ª–µ–ø–∫–∏", "–ë–æ–ª—Ç—ã", 
            "–ì–∞–π–∫–∏", "–®–∞–π–±—ã", "–®–ø–∏–ª—å–∫–∏", "–ö—Ä–æ–Ω—à—Ç–µ–π–Ω—ã", "–ù–∞–≥–µ–ª—å"
        ]
        
        for category in categories:
            catalog_url = f"{self.base_url}/catalog?tfc_storepartuid[606177230]={category}&tfc_div=:::"
            pages.append(catalog_url)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print("üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞...")
        discovered_pages = self.discover_all_pages()
        pages.extend(discovered_pages)
        
        # –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        return list(set(pages))
    
    def download_all_images(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –í–°–ï–• –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–µ"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –í–°–ï–• –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å {self.base_url}")
        print(f"üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {self.image_folder.absolute()}")
        
        pages = self.get_all_pages()
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(pages)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞
        image_groups = {}
        
        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        for i, page_url in enumerate(pages, 1):
            print(f"\nüîç [{i}/{len(pages)}] –ü–∞—Ä—Å–∏–Ω–≥: {page_url}")
            html_content = self.get_page_content(page_url)
            
            if html_content:
                image_urls = self.extract_images_from_html(html_content, page_url)
                print(f"   –ù–∞–π–¥–µ–Ω–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_urls)}")
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                for url in image_urls:
                    # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –≥—Ä—É–ø–ø—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑–æ–≤–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    base_name = re.sub(r'[?&].*', '', url)  # —É–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    base_name = re.sub(r'_\d+x\d+', '', base_name)  # —É–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
                    base_name = re.sub(r'(thumb_|small_|medium_)', '', base_name)  # —É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã
                    
                    if base_name not in image_groups:
                        image_groups[base_name] = []
                    image_groups[base_name].append(url)
                
                time.sleep(0.8)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_groups)}")
        total_variants = sum(len(variants) for variants in image_groups.values())
        print(f"üìä –í—Å–µ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_variants}")
        
        # –°–∫–∞—á–∞—Ç—å –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        print(f"\n‚¨áÔ∏è  –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–µ...")
        downloaded_count = 0
        
        for i, (base_name, variants) in enumerate(image_groups.items(), 1):
            print(f"\n[{i}/{len(image_groups)}] –ì—Ä—É–ø–ø–∞: {os.path.basename(base_name)}")
            print(f"   –í–∞—Ä–∏–∞–Ω—Ç–æ–≤: {len(variants)}")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∫–∞—á–µ—Å—Ç–≤–∞
            sorted_variants = self.sort_by_quality_priority(variants)
            
            if self.try_download_best_quality(sorted_variants):
                downloaded_count += 1
            
            time.sleep(0.3)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏
        
        print(f"\n‚úÖ –ó–ê–í–ï–†–®–ï–ù–û! –°–∫–∞—á–∞–Ω–æ {downloaded_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–µ")
        print(f"üìÅ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.image_folder.absolute()}")
        return downloaded_count
    
    def sort_by_quality_priority(self, variants):
        """–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∫–∞—á–µ—Å—Ç–≤–∞"""
        def quality_score(url):
            score = 0
            url_lower = url.lower()
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            if 'original' in url_lower or 'full' in url_lower:
                score += 1000
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–∂–∞—Ç—ã–µ –≤–µ—Ä—Å–∏–∏
            if 'thumb' in url_lower:
                score -= 500
            if 'small' in url_lower:
                score -= 400
            if 'medium' in url_lower:
                score -= 200
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ä–∞–∑–º–µ—Ä—É –≤ URL
            size_matches = re.findall(r'(\d+)x(\d+)', url)
            if size_matches:
                width, height = map(int, size_matches[-1])
                score += width * height / 1000  # –±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä = –±–æ–ª—å—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∞–º –±–µ–∑ –ø–æ—Ç–µ—Ä—å
            if url_lower.endswith('.png'):
                score += 50
            elif url_lower.endswith('.jpg') or url_lower.endswith('.jpeg'):
                score += 30
            elif url_lower.endswith('.webp'):
                score += 20
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∂–∞—Ç–∏—è
            if 'quality=' in url and 'quality=100' not in url:
                score -= 100
            if 'compress' in url_lower:
                score -= 150
            
            return score
        
        return sorted(variants, key=quality_score, reverse=True)

def main():
    downloader = AdvancedImageDownloader()
    downloader.download_all_images()

if __name__ == "__main__":
    main()
